---
layout:     post
title:      "cs294 hw2"
date:       2018-09-18 12:00:00
author:     "ZhY"
header-img: "img/post-bg-basic.jpg"
header-mask: 0.3
catalog:    true
tags:
    - 强化学习
---

### Policy gradient

在hw1的模仿学习中我们将策略$$\pi_{\theta}$$看作一个网络，他的输入是状态$$s$$，输出是动作$$a$$或者$$a$$的分布，即$$a=\pi_\theta(s)$$。但是由于模仿学习需要提供大量数据，并且可能无法提供某种动作信息，以及训练过程中会误差累计等问题，我们需要拓展新的思路。

我们已经知道强化学习的算法流程是：运行策略生成样本===>利用样本估计回报函数（对应具体任务）===>根据回报函数更新策略===>运行策略生成样本……

如果我们使用深度学习网络进行策略更新，那么只需要将第三步改为：根据回报函数使用梯度更新策略网络

那么针对于目标函数我们进行求导。求导过程可以参照[强化学习传说：第四章 策略梯度][1]以及[cs294 note4][2]中的相关内容。

其中比较重点的是在进行策略梯度下降的时候会面临高方差的问题，这种情况下有两种解决办法：

第一种是reward_to_go的方法，即只计算现在时刻之后的累计回报。因为我们现在的策略是和之前已获得的回报是没有关系的，在这种情况下如果计算了之前的回报，就会带来方差。因此采用reward_to_go的方式可以有效减少这里的方差。

![](/img/in-post/cs294_hw2/001.png)

![](/img/in-post/cs294_hw2/002.png)

第二种是通过baseline的方式，引入常数项$$b$$来减少方差。一般来说$$b$$可以使用平均回报来计算，但是这样$$b$$虽然是无偏的但仍会带来一点方差（实际上这样做虽然会带来一点方差，但是总体而言方差还是减小的）。

### Q和V

刚才所说的两种方法综合起来，通过reward_to_go获得的回报其实也就是$$Q(s,a)$$，而baseline的常数项$$b$$是其均值，也就是$$V$$值函数。所以我们得到了$$Adv(s,a)=Q(s,a)-V(s,a)$$，这里的$$Adv$$被称为优势函数，即$$Q$$值函数比平均来说优了多少。这点也和AC算法产生了关系，具体内容可以参照[强化学习传说：第四章 策略梯度][1]。

### 代码部分

#### 策略网络部分

1.策略网络的输出分为了离散和连续两种状态：在离散状态下，策略网络的输出即每个动作的logits。在连续状态下，策略网络的输出是关于动作的高斯分布，其中均值是策略网络的输出，方差是一个可训练的变量。

2.生成样本时：离散状态下，生成样本直接根据logits进行采样(tf.multinomial)。连续状态下，生成样本要根据高斯分布生成样本，但是问题在于这样直接生成样本无法进行梯度回传，在进行梯度更新的时候会出现问题。这种情况下，我们采用representation的方式进行样本生成。即如果随机变量$$x$$符合$$N(\mu,\Sigma)$$，那么$$x~\mu+\sigma*z, z~N(0,1)$$。这样采样时只需要从$$z$$中采样，梯度就可以正常回传给$$\mu$$这部分了。

**这里的representation的方法与VAE中解决梯度的方式是一样的，可以参考VAE理解。不得不说很多深度学习的东西都是相辅相成的**

3.计算log probability：离散状态直接采用交叉熵进行计算，连续状态下使用tensorflow封装好的多元正态分布的函数log_prob计算   tf.contrib.distributions.MultivariateNormalDiag(loc=mean, scale_diag=tf.exp(logstd)).log_prob(action)

需要提一下的是在[cs294 note4][2]提到了





---
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
